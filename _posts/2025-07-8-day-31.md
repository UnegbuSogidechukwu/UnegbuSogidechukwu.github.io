---
layout: post
title: "Day 31 – Week-7, Day 2 of CEAMLS "
date: 2025-07-8
author: Sogidechukwu Unegbu
permalink: /day31.html
tags: ["CEAMLS", "Kaggle", "intro to deep learning", "Neural Network"]

what_i_learned: |  
  Today I continued the Kaggle course **Introduction to deep learning**. While progressing through the course I leearnt the following:
    - Learning rate and size of minibatchs has the most effects in neral network training.
    - Early stopping. this is interrupting the training to help keep an eye on the model.
    - The meaning of overfitting and underfitting in terms of signal and noise
    - A signal helps our model make prediction from new data.
    - Noise are parts of data that look useful but is not really useful. 

  Making progress on my knowledge of Neural network
    
  I also had a conversation with Mentors with a review of the presentation
    - reviewed speech of members (pronounciation and projection)
    - Went over our confidence levels
    - Talked about interpolation of datapoints
    
  
blockers: |
  None
  
reflection: |
  Today’s Kaggle course taught me how learning rates and batch sizes critically impact neural network training, while early stopping prevents overfitting. I learned to distinguish signal (meaningful patterns) from noise (misleading data) in model predictions. Our mentor session improved my presentation skills, focusing on speech clarity and data interpretation. These experiences strengthened both my technical knowledge and communication abilities. I would look into the assignement given today (learning about interpolation)


  
  

  
   
---
